"""SageMaker inference wrapper for Textract LayoutLM Word Classification"""

# Python Built-Ins:
import json
import logging
import os
import pickle
import zipfile

# External Dependencies:
import numpy as np
# Sadly, LayoutLMTokenizer doesn't seem to work with AutoTokenizer as it expects config.json, not
# tokenizer_config.json:
from transformers import LayoutLMTokenizer, AutoConfig, AutoModelForTokenClassification
import torch
from torch.nn import CrossEntropyLoss

# Local Dependencies:
from data import TextractLayoutLMDataset
import trp

logger = logging.getLogger("infcustom")
logger.info("Loading custom inference handlers")
# If you need to debug this script and aren't seeing any logging in CloudWatch, try setting the following on
# your PyTorchModel to force flushing log calls through before crashing: env={ "PYTHONUNBUFFERED": "1" }


def input_fn(input_bytes, content_type):
    """Deserialize Textract JSON to a tuple of (doc_json, trp_doc)

    Both the parsed JSON and the Textract Response Parser result are returned because the latter provides an
    easy interface to update the former in-place: So the mutated JSON can be returned as endpoint response.

    Input must be 'application/json' data as generated by Amazon Textract or the Amazon Textractor CLI tool.
    """
    logger.info(f"Received request of type:{content_type}")
    if content_type == "application/json":
        doc_json = json.loads(input_bytes)
        return doc_json, trp.Document(doc_json)
    else:
        raise ValueError("Content type must be application/json")


# No custom [output_fn(data, accept) -> serialized, accept] required as predict_fn returns JSONable result.
# def output_fn(prediction_output, accept):
#     if accept == "application/json":
# #         logger.info(prediction_output)
#         return prediction_output, "application/json"
#     else:
#         raise ValueError('Accept header must be application/json')


def model_fn(model_dir):
    """Load model artifacts from 'model.pth' zip archive in model_dir

    Returns
    -------
    model : transformers.AutoModelForTokenClassification
        Core HuggingFace Transformers model, initialized for evaluation and loaded to GPU if present
    tokenizer : transformers.LayoutLMTokenizer
        Core HuggingFace Tokenizer for LayoutLM as serialized in the model.pth
    config : transformers.AutoConfig
        Would be useful if we could use transformers.pipeline, but sadly cannot
    device : torch.device
        Indicating which device the model was loaded to
    """
    logger.info(f"Loading model archive")
    # To work around https://github.com/pytorch/serve/pull/814 our 'model.pth' is actually a zip file, so
    # we first need to extract it to load the other files into model_dir:
    try:
        with zipfile.ZipFile(os.path.join(model_dir, "model.pth"), "r") as mdlzip:
            mdlzip.extractall(model_dir)
    except BadZipFile as e:
        logger.error(
            f"Failed to load 'model.pth': which should be a zip archive containing various model files"
        )
        raise e
    logger.info(f"Loaded model archive contents: {os.listdir(model_dir)}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = LayoutLMTokenizer.from_pretrained(os.path.join(model_dir, "tokenizer"))
    model = AutoModelForTokenClassification.from_pretrained(model_dir)
    model.eval()
    model.to(device)
    config = AutoConfig.from_pretrained(model_dir)
    logger.info(f"Model loaded")
    return model, tokenizer, config, device


def predict_fn(input_data, model):
    """Classify WORD blocks on a Textract result according to a LayoutLMForTokenClassification model

    Parameters
    ----------
    input_data : Tuple[Union[List, Dict], trp.Document]
        Parsed JSON of Textract result, plus the same document loaded with Textract Response Parser.
    model : Tuple[AutoModelForTokenClassification, LayoutLMTokenizer, AutoConfig, torch.device]
        The core token classification model, tokenizer, config (not used) and PyTorch device.

    Returns
    -------
    doc_json : Union[List, Dict]
        Input Textract JSON with WORD blocks annotated with additional properties describing their
        classification according to the model: PredictedClass (integer ID of highest-scoring class),
        ClassificationProbabilities (list of floats scoring confidence for each possible class), and
        PredictedClassConfidence (float confidence of highest-scoring class).
    """
    trained_model, tokenizer, config, device = model
    doc_json, trp_doc = input_data
    with torch.no_grad():
        # We can't use pipeline/TextClassificationPipeline, because LayoutLMForTokenClassification has been
        # implemented such that the bbox input is separate and *optional*, and doesn't come from the tokenizer!
        # So instead the logic here is heavily inspired by the pipeline but with some customizations:
        # https://github.com/huggingface/transformers/blob/f51188cbe74195c14c5b3e2e8f10c2f435f9751a/src/transformers/pipelines/token_classification.py#L115
        # nlp = pipeline(
        #     task="token-classification",
        #     model=trained_model,
        #     config=config,
        #     tokenizer=tokenizer,
        #     framework="pt",
        # )
        tokenized = TextractLayoutLMDataset.tokenize_textract_doc(
            trp_doc,
            tokenizer,
            return_tensors="pt",
        )
        # Store the additional/diagnostic token outputs and separate them from model inputs:
        output_page_nums = np.array(tokenized["page_num"])
        del tokenized["page_num"]
        output_block_ids = np.array(tokenized["textract_block_ids"])
        del tokenized["textract_block_ids"]
        # Push model inputs to GPU if needed:
        for name in tokenized:
            tokenized[name] = tokenized[name].to(device)

        # Call the model with inputs:
        output = trained_model.forward(**tokenized)
        # output.logits is (batch_size, seq_len, n_labels)

        # Convert logits to probabilities and retrieve to numpy:
        output_probs = torch.nn.functional.softmax(output.logits, dim=-1)
        probs_cpu = output_probs.cpu() if output_probs.is_cuda else output_probs
        probs = probs_cpu.numpy()

        # Flatten the batch dimension:
        flat_block_ids = output_block_ids.flatten()
        token_page_nums = output_page_nums[..., np.newaxis].repeat(output_block_ids.shape[1], axis=1)
        flat_page_nums = token_page_nums.flatten()
        flat_probs = probs.reshape(-1, 4)

        # Filter out examples that don't correspond to actual Textract 'blocks' (special tokens, pad, etc):
        real_token_indices = flat_block_ids != None
        filtered_block_ids = flat_block_ids[real_token_indices]
        filtered_page_nums = flat_page_nums[real_token_indices]
        filtered_probs = flat_probs[real_token_indices, ...]

        # Average probabilities by Textract block ID:
        # First convert string block IDs to index numbers with a vocabulary:
        id_lookup, filtered_block_numeric_ids = np.unique(filtered_block_ids, return_inverse=True)
        # Then take the average by adding up the scores then dividing through by count
        avg_probs = np.zeros((len(id_lookup), *filtered_probs.shape[1:]), dtype=filtered_probs.dtype)
        np.add.at(avg_probs, filtered_block_numeric_ids, filtered_probs)
        avg_out = avg_probs / np.bincount(filtered_block_numeric_ids).astype(float)[:, np.newaxis]

        # Use the trp_doc to annotate each WORD block with its classification (which will also update
        # doc_json in place):
        for ixblock, block_id in enumerate(id_lookup):
            probs = avg_out[ixblock, ...]
            block = trp_doc.getBlockById(block_id)
            # Remember numpy dtypes may not be JSON serializable, so convert to native types:
            block["ClassificationProbabilities"] = probs.tolist()
            block["PredictedClass"] = int(np.argmax(probs))
            block["PredictedClassConfidence"] = float(probs[block["PredictedClass"]])

        # For endpoint output, return the updated doc_json with annotations on word blocks:
        return doc_json
